import streamlit as st
import pandas as pd
import time
import json
from bs4 import BeautifulSoup
import random
import re

# --- Selenium ç›¸å…³å¯¼å…¥ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (ç®€åŒ–äº†æ—¥å¿—è¾“å‡º) ---
def scrape_homedepot_with_selenium(query, max_pages_to_scrape):
    """
    ä½¿ç”¨æ ‡å‡†Seleniumé©±åŠ¨æµè§ˆå™¨ï¼Œé€šè¿‡è§£æ__APOLLO_STATE__æ•°æ®å—å¹¶æ™ºèƒ½ç¿»é¡µï¼ŒæŠ“å–å®Œæ•´æ•°æ®ã€‚

    Args:
        query (str): æœç´¢å…³é”®è¯ã€‚
        max_pages_to_scrape (int): ç”¨æˆ·æŒ‡å®šçš„æœ€å¤§æŠ“å–é¡µæ•°ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰é¡µé¢å•†å“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    all_results = []
    status_placeholder = st.empty()
    driver = None
    
    try:
        status_placeholder.info(f"ğŸš€ æ­£åœ¨é…ç½®æ ‡å‡†ç‰ˆæµè§ˆå™¨...")
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument(f"--user-data-dir=/tmp/selenium_user_data_{int(time.time())}")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
        
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)
        
        search_url = f"https://www.homedepot.com/s/{query.replace(' ', '%20')}"
        status_placeholder.info(f"ğŸ•µï¸ æµè§ˆå™¨å·²å¯åŠ¨ï¼Œæ­£åœ¨è®¿é—®åˆå§‹é¡µé¢...")
        driver.get(search_url)

        current_page = 1
        
        while True:
            if current_page > max_pages_to_scrape:
                status_placeholder.info(f"å·²è¾¾åˆ°è®¾å®šçš„æœ€å¤§æŠ“å–é¡µæ•° ({max_pages_to_scrape})ï¼Œä»»åŠ¡ç»“æŸã€‚")
                break

            status_text = f"â³ æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ | å·²æŠ“å– {len(all_results)} ä¸ªå•†å“..."
            status_placeholder.info(status_text)
            
            wait = WebDriverWait(driver, 30)
            
            try:
                # 1. ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'nav[aria-label="Pagination Navigation"]')))
                
                # 2. ä»HTMLä¸­æå–APOLLO_STATE
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                script_tag = soup.find('script', string=re.compile(r"window\.__APOLLO_STATE__"))
                
                if not script_tag:
                    st.warning(f"ç¬¬ {current_page} é¡µæœªæ‰¾åˆ° __APOLLO_STATE__ æ•°æ®å—ã€‚")
                    break

                # 3. ç²¾ç¡®æå–å¹¶è§£æJSON
                match = re.search(r'window\.__APOLLO_STATE__\s*=\s*({.*});', script_tag.string)
                if not match:
                    st.warning(f"ç¬¬ {current_page} é¡µæ— æ³•ä»è„šæœ¬ä¸­æ­£ç¡®æå–JSONæ•°æ®ã€‚")
                    break
                
                json_text = match.group(1)
                apollo_data = json.loads(json_text)
                
                products_list = []
                for key in apollo_data:
                    if isinstance(apollo_data[key], dict) and 'products' in str(apollo_data[key]):
                        for sub_key, sub_value in apollo_data[key].items():
                            if isinstance(sub_value, list) and sub_value and isinstance(sub_value[0], dict) and sub_value[0].get('__ref'):
                                product_refs = sub_value
                                for ref in product_refs:
                                    product_id = ref.get('__ref')
                                    if product_id and product_id in apollo_data:
                                        products_list.append(apollo_data[product_id])
                                if products_list: break
                        if products_list: break

                if not products_list:
                    st.info(f"ç¬¬ {current_page} é¡µæœªè§£æåˆ°äº§å“ï¼ŒæŠ“å–ç»“æŸã€‚")
                    break
                
                for product in products_list:
                    name = product.get('identifiers', {}).get('productLabel', 'N/A')
                    pricing_key = next((k for k in product if k.startswith('pricing')), None)
                    pricing_info = product.get(pricing_key, {}) if pricing_key else {}
                    original_price = pricing_info.get('original')
                    current_price = pricing_info.get('value')
                    link = "https://www.homedepot.com" + product.get('identifiers', {}).get('canonicalUrl', '#')
                    image_url = product.get('media', {}).get('images', [{}])[0].get('url')
                    if image_url:
                        image_url = image_url.replace("<SIZE>", "400")

                    if name != 'N/A':
                        all_results.append({
                            'name': name, 'current_price': current_price, 'original_price': original_price,
                            'link': link, 'image_url': image_url or 'https://placehold.co/100x100/e2e8f0/333333?text=No+Image'
                        })

            except TimeoutException:
                status_placeholder.error(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆç¬¬ {current_page} é¡µï¼‰ã€‚")
                st.image(driver.get_screenshot_as_png(), caption="è¶…æ—¶å¿«ç…§")
                break 

            # 4. å¯»æ‰¾ä¸‹ä¸€é¡µçš„URLå¹¶å¯¼èˆª
            try:
                next_page_element = driver.find_element(By.CSS_SELECTOR, 'a[aria-label="Skip to Next Page"]')
                next_page_url = next_page_element.get_attribute('href')
                driver.get(next_page_url)
                current_page += 1
                time.sleep(random.uniform(1.5, 3.5))
            except NoSuchElementException:
                status_placeholder.success(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ŒæŠ“å–å®Œæˆï¼å…±å¤„ç† {current_page} é¡µã€‚")
                break 
    
    except Exception as e:
        status_placeholder.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        if driver:
            driver.quit()
            
    return all_results

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="åœ¨çº¿å•†å“ä¿¡æ¯å·¥å…·", layout="wide")
st.title("ğŸ›’ åœ¨çº¿å•†å“ä¿¡æ¯å·¥å…·")

search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", "milwaukee")

# --- æ–°çš„äº¤äº’UI ---
col1, col2 = st.columns([1, 4])
with col1:
    limit_pages = st.checkbox("é™åˆ¶é¡µæ•°", value=False)
with col2:
    if limit_pages:
        max_pages_to_scrape = st.number_input("è¦æŠ“å–çš„é¡µæ•°:", min_value=1, max_value=50, value=3, key="max_pages_limited")
    else:
        max_pages_to_scrape = 999  # è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„æ•°ä»£è¡¨â€œå…¨éƒ¨â€
        st.write("å°†æŠ“å–æ‰€æœ‰å¯ç”¨çš„é¡µé¢ã€‚")


if st.button("ğŸš€ å¼€å§‹æœç´¢"):
    if not search_query:
        st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
    else:
        all_scraped_data = scrape_homedepot_with_selenium(search_query, max_pages_to_scrape)

        if all_scraped_data:
            st.success(f"ğŸ‰ **ä»»åŠ¡ç»“æŸï¼å…±è·å¾— {len(all_scraped_data)} æ¡å•†å“ä¿¡æ¯ï¼**")
            
            full_df = pd.DataFrame(all_scraped_data)
            duplicates_mask = full_df.duplicated(subset=['name'], keep='first')
            unique_df = full_df[~duplicates_mask]
            duplicate_df = full_df[duplicates_mask]
            
            st.info(f"å»é‡åå‰©ä½™ {len(unique_df)} æ¡ç‹¬ç«‹å•†å“ä¿¡æ¯ã€‚")
            
            st.subheader("ç‹¬ç«‹å•†å“ä¿¡æ¯")
            display_unique_data = [{
                "å›¾ç‰‡": row['image_url'],
                "å•†å“åç§°": row['name'],
                "åŸä»·": f"${row['original_price']}" if pd.notna(row['original_price']) and row['original_price'] != row['current_price'] else " ",
                "ç°å”®ä»·": f"${row['current_price']}" if pd.notna(row['current_price']) else 'N/A',
                "é“¾æ¥": row['link']
            } for _, row in unique_df.iterrows()]
            
            st.dataframe(
                display_unique_data,
                column_config={
                    "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                    "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                    "åŸä»·": st.column_config.TextColumn("åŸä»·", width="small"),
                    "ç°å”®ä»·": st.column_config.TextColumn("ç°å”®ä»·", width="small"),
                    "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                }, hide_index=True, use_container_width=True)

            if not duplicate_df.empty:
                with st.expander(f"æŸ¥çœ‹ {len(duplicate_df)} æ¡é‡å¤çš„å•†å“ä¿¡æ¯"):
                    st.dataframe(duplicate_df)
            
        else:
            st.error("æœªèƒ½æŠ“å–åˆ°ä»»ä½•å•†å“ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹çš„æ—¥å¿—åˆ†æåŸå› ã€‚")
