import streamlit as st
import pandas as pd
import time
import json
from bs4 import BeautifulSoup

# --- Selenium ç›¸å…³å¯¼å…¥ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (æœ€ç»ˆç‰ˆ) ---
def scrape_homedepot_page(query, page_num=1):
    """
    ä½¿ç”¨Seleniumé©±åŠ¨æµè§ˆå™¨ï¼Œä»å•ä¸ªæœç´¢ç»“æœé¡µé¢æŠ“å–å®Œæ•´çš„å•†å“æ•°æ®ã€‚
    è¿™ä¸ªç‰ˆæœ¬ä¸“ä¸ºåœ¨Streamlitäº‘æœåŠ¡å™¨ç­‰Linuxç¯å¢ƒéƒ¨ç½²è€Œä¼˜åŒ–ã€‚

    Args:
        query (str): æœç´¢å…³é”®è¯ã€‚
        page_num (int): è¦æŠ“å–çš„é¡µç ã€‚

    Returns:
        list: åŒ…å«è¯¥é¡µé¢å•†å“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œæˆ–åœ¨å‡ºé”™æ—¶è¿”å›Noneã€‚
    """
    search_url = f"https://www.homedepot.com/s/{query.replace(' ', '%20')}?page={page_num}"
    
    st.write("---")
    st.write(f"âš™ï¸ **æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...**")
    
    driver = None
    try:
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument(f"--user-data-dir=/tmp/selenium_user_data_{int(time.time())}_{page_num}")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)
        
        st.write(f"  > æµè§ˆå™¨æ­£åœ¨è®¿é—®: {search_url}")
        driver.get(search_url)

        # ç­‰å¾… __NEXT_DATA__ è„šæœ¬æ ‡ç­¾åŠ è½½å®Œæˆ
        st.write("  > ç­‰å¾…é¡µé¢åŠ¨æ€æ•°æ®åŠ è½½...")
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.ID, "__NEXT_DATA__"))
        )
        st.success("   > å…³é”®æ•°æ®å·²åŠ è½½ï¼")

        html_source = driver.page_source
        soup = BeautifulSoup(html_source, 'html.parser')
        
        script_tag = soup.find('script', {'id': '__NEXT_DATA__', 'type': 'application/json'})
        
        if not script_tag:
            st.error("é”™è¯¯ï¼šæœªèƒ½æ‰¾åˆ° '__NEXT_DATA__' æ•°æ®å—ã€‚")
            return []

        json_data = json.loads(script_tag.string)
        
        products_list = []
        page_props = json_data.get('props', {}).get('pageProps', {})
        if page_props.get('search', {}).get('contentLayouts'):
            content_layouts = page_props['search']['contentLayouts']
            for layout in content_layouts:
                if isinstance(layout, dict) and layout.get('type') == 'PRODUCT_POD' and layout.get('products'):
                    products_list.extend(layout.get('products', []))
        
        if not products_list:
            st.warning(f"ç¬¬ {page_num} é¡µæœªè§£æåˆ°äº§å“ï¼Œå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µã€‚")
            return []

        page_results = []
        for product in products_list:
            item_data = product.get('item', product)
            name = item_data.get('productLabel', 'N/A')
            price_info = item_data.get('pricing', {})
            price = price_info.get('specialPrice', {}).get('price') or price_info.get('originalPrice', {}).get('price')
            link = 'https://www.homedepot.com' + item_data.get('url', '#')
            image_url = item_data.get('media', {}).get('images', [{}])[0].get('url')

            if name != 'N/A':
                page_results.append({
                    'name': name, 'price': price, 'link': link, 
                    'image_url': image_url or 'https://placehold.co/100x100/e2e8f0/333333?text=No+Image'
                })
        
        st.write(f"  > **æˆåŠŸè§£æåˆ° {len(page_results)} ä¸ªäº§å“!**")
        return page_results

    except TimeoutException:
        st.error(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆç¬¬ {page_num} é¡µï¼‰ã€‚å¯èƒ½æ˜¯è¢«åçˆ¬è™«æœºåˆ¶æ‹¦æˆªã€‚")
        return None
    except Exception as e:
        st.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None
    finally:
        if driver:
            driver.quit()

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="Home Depot Selenium çˆ¬è™«", layout="wide")
st.title("ğŸ›’ Home Depot å•†å“æŠ“å–å·¥å…· (æœ€ç»ˆç‰ˆ)")

search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", "milwaukee")
num_pages = st.number_input("è¦æŠ“å–çš„é¡µæ•°:", min_value=1, max_value=10, value=2, help="å»ºè®®ä¸è¦ä¸€æ¬¡æ€§æŠ“å–å¤ªå¤šé¡µï¼Œä»¥é¿å…è¢«å°ç¦ã€‚")

if st.button("ğŸš€ ä½¿ç”¨ Selenium å¼€å§‹æœç´¢"):
    if not search_query:
        st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
    else:
        all_scraped_data = []
        with st.spinner(f"æ­£åœ¨å¯åŠ¨Seleniumå¹¶æœç´¢ '{search_query}'..."):
            for i in range(1, num_pages + 1):
                page_data = scrape_homedepot_page(search_query, i)
                if page_data is None:
                    st.error("æŠ“å–è¿‡ç¨‹ä¸­æ–­ã€‚")
                    break
                if not page_data:
                    st.info("å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ŒæŠ“å–ç»“æŸã€‚")
                    break
                all_scraped_data.extend(page_data)
                if i < num_pages:
                    time.sleep(2) # ç¿»é¡µä¹‹é—´ç¤¼è²Œæ€§ç­‰å¾…

        if all_scraped_data:
            st.success(f"ğŸ‰ **æŠ“å–å®Œæˆï¼å…±è·å¾— {len(all_scraped_data)} æ¡å•†å“ä¿¡æ¯ï¼**")
            
            df = pd.DataFrame(all_scraped_data).drop_duplicates(subset=['name'])
            st.info(f"å»é‡åå‰©ä½™ {len(df)} æ¡ç‹¬ç«‹å•†å“ä¿¡æ¯ã€‚")
            
            display_df_data = [{
                "å›¾ç‰‡": row['image_url'],
                "å•†å“åç§°": row['name'],
                "ä»·æ ¼": f"${row['price']}" if row.get('price') else 'N/A',
                "é“¾æ¥": row['link']
            } for _, row in df.iterrows()]
            
            st.dataframe(
                display_df_data,
                column_config={
                    "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                    "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                    "ä»·æ ¼": st.column_config.TextColumn("ä»·æ ¼", width="small"),
                    "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                }, hide_index=True, use_container_width=True)
        else:
            st.error("æœªèƒ½æŠ“å–åˆ°ä»»ä½•å•†å“ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹çš„æ—¥å¿—åˆ†æåŸå› ã€‚")
        
st.markdown("---")
st.markdown("æŠ€æœ¯è¯´æ˜ï¼šæ­¤åº”ç”¨ä½¿ç”¨ `Selenium` é©±åŠ¨åœ¨åå°è¿è¡Œçš„ `Chrome` æµè§ˆå™¨è·å–é¡µé¢æºç ï¼Œå†ç”± `BeautifulSoup` å’Œ `json` è§£ææ•°æ®ã€‚")
