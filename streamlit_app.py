import streamlit as st
import pandas as pd
import time
import json
from bs4 import BeautifulSoup

# --- Selenium ç›¸å…³å¯¼å…¥ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (æ™ºèƒ½URLç¿»é¡µ) ---
def scrape_homedepot_with_selenium(query, max_pages_to_scrape):
    """
    ä½¿ç”¨Seleniumé©±åŠ¨æµè§ˆå™¨ï¼Œé€šè¿‡æå–å¹¶å¯¼èˆªåˆ°åˆ†é¡µå™¨ä¸­çš„URLè¿›è¡Œæ™ºèƒ½ç¿»é¡µæŠ“å–ã€‚

    Args:
        query (str): æœç´¢å…³é”®è¯ã€‚
        max_pages_to_scrape (int): æœ¬æ¬¡è¿è¡Œæœ€å¤šæŠ“å–çš„é¡µé¢æ•°ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰é¡µé¢å•†å“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    search_url = f"https://www.homedepot.com/s/{query.replace(' ', '%20')}"
    all_results = []
    
    st.write("---")
    st.write("âš™ï¸ **Selenium è‡ªåŠ¨åŒ–æµç¨‹å¯åŠ¨...**")
    
    driver = None
    try:
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument(f"--user-data-dir=/tmp/selenium_user_data_{int(time.time())}")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)
        
        st.write(f"  > æµè§ˆå™¨æ­£åœ¨è®¿é—®åˆå§‹é¡µé¢: {search_url}")
        driver.get(search_url)

        current_page = 1
        while True:
            st.write(f"--- \nâš™ï¸ **æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ...**")
            wait = WebDriverWait(driver, 30)
            
            try:
                # 1. ç­‰å¾…å½“å‰é¡µæ•°æ®åŠ è½½
                st.write("  > ç­‰å¾…ç›®æ ‡æ•°æ®è„šæœ¬åŠ è½½...")
                wait.until(EC.presence_of_element_located((By.ID, "thd-helmet__script--browseSearchStructuredData")))
                st.success("   > ç›®æ ‡æ•°æ®è„šæœ¬å·²åŠ è½½ï¼")

                # 2. è§£ææ•°æ®
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                script_tag = soup.find('script', {'id': 'thd-helmet__script--browseSearchStructuredData', 'type': 'application/ld+json'})
                
                if not script_tag:
                    st.warning(f"ç¬¬ {current_page} é¡µæœªæ‰¾åˆ°æ•°æ®ï¼Œå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µã€‚")
                    break

                json_data = json.loads(script_tag.string)
                products_list = json_data[0].get('mainEntity', {}).get('offers', {}).get('itemOffered', [])
                
                if not products_list:
                    st.info(f"ç¬¬ {current_page} é¡µæœªè§£æåˆ°äº§å“ï¼ŒæŠ“å–ç»“æŸã€‚")
                    break
                
                st.write(f"  > **æˆåŠŸè§£æåˆ° {len(products_list)} ä¸ªäº§å“!**")
                for product in products_list:
                    name = product.get('name', 'N/A')
                    offers = product.get('offers', {})
                    price = offers.get('price', 'N/A') if isinstance(offers, dict) else 'N/A'
                    link = offers.get('url', '#') if isinstance(offers, dict) else '#'
                    image_url = product.get('image')

                    if name != 'N/A':
                        all_results.append({
                            'name': name, 'price': price, 'link': link, 
                            'image_url': image_url or 'https://placehold.co/100x100/e2e8f0/333333?text=No+Image'
                        })

            except TimeoutException:
                st.error(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆç¬¬ {current_page} é¡µï¼‰ã€‚")
                break 

            # 3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶
            if current_page >= max_pages_to_scrape:
                st.info(f"å·²è¾¾åˆ°è®¾å®šçš„æœ€å¤§æŠ“å–é¡µæ•° ({max_pages_to_scrape})ã€‚")
                break
            
            # 4. å¯»æ‰¾ä¸‹ä¸€é¡µçš„URLå¹¶å¯¼èˆª
            try:
                st.write("  > æ­£åœ¨å¯»æ‰¾ä¸‹ä¸€é¡µçš„é“¾æ¥...")
                # ä¼˜å…ˆå¯»æ‰¾ "Skip to Next Page" çš„æŒ‰é’®ï¼Œå®ƒçš„URLæ˜¯æœ€å‡†ç¡®çš„
                next_page_element = driver.find_element(By.CSS_SELECTOR, 'a[aria-label="Skip to Next Page"]')
                next_page_url = next_page_element.get_attribute('href')
                
                st.write(f"  > æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œæ­£åœ¨å¯¼èˆªè‡³ç¬¬ {current_page + 1} é¡µ...")
                driver.get(next_page_url)
                current_page += 1

            except NoSuchElementException:
                st.success("âœ… æœªæ‰¾åˆ° 'Next' æŒ‰é’®ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µï¼ŒæŠ“å–ç»“æŸã€‚")
                break 
    
    except Exception as e:
        st.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        st.write("--- \nğŸ **æ‰€æœ‰æŠ“å–ä»»åŠ¡å®Œæˆï¼Œå…³é—­æµè§ˆå™¨é©±åŠ¨ã€‚**")
        if driver:
            driver.quit()
            
    return all_results

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="Home Depot Selenium çˆ¬è™«", layout="wide")
st.title("ğŸ›’ Home Depot å•†å“æŠ“å–å·¥å…· (æ™ºèƒ½ç¿»é¡µç‰ˆ)")

search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", "milwaukee")
max_pages = st.number_input("æœ€å¤šæŠ“å–çš„é¡µæ•°:", min_value=1, max_value=10, value=3, help="è®¾å®šä¸€ä¸ªæŠ“å–ä¸Šé™ï¼Œä»¥é¿å…è¿è¡Œæ—¶é—´è¿‡é•¿æˆ–è¢«å°ç¦ã€‚")

if st.button("ğŸš€ ä½¿ç”¨ Selenium å¼€å§‹æœç´¢"):
    if not search_query:
        st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
    else:
        all_scraped_data = []
        with st.spinner(f"æ­£åœ¨å¯åŠ¨Seleniumå¹¶æœç´¢ '{search_query}'..."):
            all_scraped_data = scrape_homedepot_with_selenium(search_query, max_pages)

        if all_scraped_data:
            st.success(f"ğŸ‰ **æŠ“å–å®Œæˆï¼å…±è·å¾— {len(all_scraped_data)} æ¡å•†å“ä¿¡æ¯ï¼**")
            
            df = pd.DataFrame(all_scraped_data).drop_duplicates(subset=['name'])
            st.info(f"å»é‡åå‰©ä½™ {len(df)} æ¡ç‹¬ç«‹å•†å“ä¿¡æ¯ã€‚")
            
            display_df_data = [{
                "å›¾ç‰‡": row['image_url'],
                "å•†å“åç§°": row['name'],
                "ä»·æ ¼": f"${row['price']}" if row.get('price') else 'N/A',
                "é“¾æ¥": row['link']
            } for _, row in df.iterrows()]
            
            st.dataframe(
                display_df_data,
                column_config={
                    "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                    "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                    "ä»·æ ¼": st.column_config.TextColumn("ä»·æ ¼", width="small"),
                    "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                }, hide_index=True, use_container_width=True)
        else:
            st.error("æœªèƒ½æŠ“å–åˆ°ä»»ä½•å•†å“ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹çš„æ—¥å¿—åˆ†æåŸå› ã€‚")
        
st.markdown("---")
st.markdown("æŠ€æœ¯è¯´æ˜ï¼šæ­¤åº”ç”¨é€šè¿‡ `Selenium` æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œä»åˆ†é¡µå™¨ä¸­æå–URLè¿›è¡Œæ™ºèƒ½ç¿»é¡µï¼Œå¹¶ä»æ¸²æŸ“åçš„ `ld+json` è„šæœ¬ä¸­æå–æ•°æ®ã€‚")
