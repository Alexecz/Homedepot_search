import streamlit as st
import pandas as pd
import time
import json
from bs4 import BeautifulSoup

# --- Selenium ç›¸å…³å¯¼å…¥ ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (é€‚ç”¨äºæœåŠ¡å™¨ç¯å¢ƒ) ---
def scrape_homedepot_with_selenium(query):
    """
    ä½¿ç”¨Seleniumé©±åŠ¨æµè§ˆå™¨æ¥æŠ“å–Home Depotçš„æœç´¢ç»“æœé¡µé¢ã€‚
    è¿™ä¸ªç‰ˆæœ¬ä¸“ä¸ºåœ¨Streamlitäº‘æœåŠ¡å™¨ç­‰Linuxç¯å¢ƒéƒ¨ç½²è€Œä¼˜åŒ–ã€‚

    Args:
        query (str): ä½ æƒ³è¦æœç´¢çš„å…³é”®è¯ã€‚

    Returns:
        list: ä¸€ä¸ªåŒ…å«å•†å“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œæˆ–åœ¨å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    page_results = []
    search_url = f"https://www.homedepot.com/s/{query.replace(' ', '%20')}"
    
    st.write("---")
    st.write("âš™ï¸ **Selenium è‡ªåŠ¨åŒ–æµç¨‹å¯åŠ¨...**")
    
    driver = None
    try:
        # --- ä¸ºæœåŠ¡å™¨ç¯å¢ƒè®¾ç½®Chromeé€‰é¡¹ ---
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")  # å¿…é¡»ä½¿ç”¨headlessæ¨¡å¼
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        # ä¸ºæ¯ä¸ªä¼šè¯åˆ›å»ºå”¯ä¸€çš„ä¸´æ—¶ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œè§£å†³ "user data directory is already in use" é”™è¯¯
        options.add_argument(f"--user-data-dir=/tmp/selenium_user_data_{int(time.time())}")
        # ä¼ªè£…æˆä¸€ä¸ªçœŸå®çš„æµè§ˆå™¨
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        st.write("1. **åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­åˆå§‹åŒ– Chrome æµè§ˆå™¨é©±åŠ¨...**")
        
        # åœ¨Streamlit Cloudä¸Šï¼ŒSeleniumä¼šè‡ªåŠ¨æŸ¥æ‰¾é€šè¿‡packages.txtå®‰è£…çš„æµè§ˆå™¨é©±åŠ¨
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)
        
        st.write(f"2. **æµè§ˆå™¨æ­£åœ¨è®¿é—®ç›®æ ‡ç½‘å€:** {search_url}")
        driver.get(search_url)

        st.write("3. **ç­‰å¾…é¡µé¢åŠ¨æ€æ•°æ® (`__NEXT_DATA__`) åŠ è½½å®Œæˆ...** (æœ€é•¿20ç§’)")
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.ID, "__NEXT_DATA__"))
        )
        st.success("   > å…³é”®æ•°æ®å·²åŠ è½½ï¼")

        st.write("4. **è·å–æ¸²æŸ“åçš„å®Œæ•´é¡µé¢ HTML...**")
        html_source = driver.page_source
        
        st.write("5. **ä½¿ç”¨ BeautifulSoup è§£æ HTML...**")
        soup = BeautifulSoup(html_source, 'html.parser')
        
        script_tag = soup.find('script', {'id': '__NEXT_DATA__', 'type': 'application/json'})
        
        if not script_tag:
            st.error("ä¸¥é‡é”™è¯¯ï¼šå³ä½¿ä½¿ç”¨Seleniumä¹Ÿæœªèƒ½æ‰¾åˆ° '__NEXT_DATA__'ã€‚")
            return []

        json_data = json.loads(script_tag.string)
        
        products_list = []
        page_props = json_data.get('props', {}).get('pageProps', {})
        if page_props.get('search', {}).get('contentLayouts'):
            content_layouts = page_props['search']['contentLayouts']
            for layout in content_layouts:
                if isinstance(layout, dict) and layout.get('type') == 'PRODUCT_POD' and layout.get('products'):
                    products_list.extend(layout.get('products', []))
        
        if not products_list:
            st.warning("åœ¨'__NEXT_DATA__'ä¸­æœªæ‰¾åˆ°äº§å“åˆ—è¡¨ã€‚")
            return []

        st.write(f"   > **æˆåŠŸè§£æåˆ° {len(products_list)} ä¸ªäº§å“æ¡ç›®!**")
        for product in products_list:
            item_data = product.get('item', product)
            name = item_data.get('productLabel', item_data.get('name', 'N/A'))
            price_info = item_data.get('pricing', {})
            price = price_info.get('specialPrice', {}).get('price') or price_info.get('originalPrice', {}).get('price')

            link = item_data.get('url', '#')
            if link.startswith('/'):
                link = 'https://www.homedepot.com' + link

            image_url = None
            images_list = item_data.get('media', {}).get('images', [])
            if images_list and isinstance(images_list, list) and len(images_list) > 0:
                image_url = images_list[0].get('url')

            if name != 'N/A':
                page_results.append({
                    'name': name,
                    'price': price,
                    'link': link,
                    'image_url': image_url if image_url else 'https://placehold.co/100x100/e2e8f0/333333?text=No+Image'
                })
        
        return page_results

    except TimeoutException:
        st.error("é¡µé¢åŠ è½½è¶…æ—¶ï¼šåœ¨20ç§’å†…æœªèƒ½æ‰¾åˆ°'__NEXT_DATA__'ã€‚å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–ç½‘ç«™åŠ è½½ç¼“æ…¢ã€‚")
        return []
    except WebDriverException as e:
        st.error(f"WebDriver é”™è¯¯: {e}")
        st.error("æ— æ³•åœ¨æœåŠ¡å™¨ä¸Šå¯åŠ¨Chromeã€‚è¯·ç¡®è®¤ 'packages.txt' æ–‡ä»¶å·²æ­£ç¡®é…ç½®å¹¶åŒ…å« 'google-chrome-stable'ã€‚")
        return []
    except Exception as e:
        st.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        import traceback
        st.text(traceback.format_exc())
        return []
    finally:
        st.write("6. **å…³é—­æµè§ˆå™¨é©±åŠ¨ï¼Œé‡Šæ”¾èµ„æºã€‚**")
        st.write("---")
        if driver:
            driver.quit()

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="Home Depot Selenium çˆ¬è™«", layout="wide")

st.title("ğŸ›’ Home Depot å•†å“æŠ“å–å·¥å…· (Seleniumç‰ˆ)")
st.markdown("""
è¿™ä¸ªç‰ˆæœ¬ä½¿ç”¨ **Selenium** æ¥é©±åŠ¨ä¸€ä¸ªçœŸå®çš„æµè§ˆå™¨è¿›è¡Œæ•°æ®æŠ“å–ï¼Œå¯ä»¥æœ‰æ•ˆç»•è¿‡ç½‘ç«™çš„å¸¸è§„åçˆ¬è™«æœºåˆ¶ï¼Œè·å–ç”±JavaScriptåŠ¨æ€åŠ è½½çš„å®Œæ•´æ•°æ®ã€‚
""")

search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ (ä¾‹å¦‚: 'milwaukee', 'dewalt'):", "milwaukee")

if st.button("ğŸš€ ä½¿ç”¨ Selenium å¼€å§‹æœç´¢"):
    if not search_query:
        st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
    else:
        with st.spinner(f"æ­£åœ¨å¯åŠ¨Seleniumå¹¶æœç´¢ '{search_query}'... (åœ¨æœåŠ¡å™¨ä¸Šé¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦1-2åˆ†é’Ÿ)"):
            scraped_data = scrape_homedepot_with_selenium(search_query)

        if scraped_data:
            st.success(f"ğŸ‰ **æŠ“å–å®Œæˆï¼å…±è·å¾— {len(scraped_data)} æ¡å•†å“ä¿¡æ¯ï¼**")
            
            df = pd.DataFrame(scraped_data).drop_duplicates(subset=['name'])
            
            display_df_data = []
            for index, row in df.iterrows():
                display_df_data.append({
                    "å›¾ç‰‡": row['image_url'],
                    "å•†å“åç§°": row['name'],
                    "ä»·æ ¼": f"${row['price']}" if row.get('price') else 'N/A',
                    "é“¾æ¥": row['link']
                })
            
            display_df = pd.DataFrame(display_df_data)

            st.dataframe(
                display_df,
                column_config={
                    "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                    "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                    "ä»·æ ¼": st.column_config.TextColumn("ä»·æ ¼", width="small"),
                    "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                },
                hide_index=True,
                use_container_width=True
            )

        else:
            st.error("æœªèƒ½æŠ“å–åˆ°ä»»ä½•å•†å“ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ä¸Šé¢çš„æ—¥å¿—è¾“å‡ºè·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
        
st.markdown("---")
st.markdown("æŠ€æœ¯è¯´æ˜ï¼šæ­¤åº”ç”¨ä¸“ä¸ºæœåŠ¡å™¨éƒ¨ç½²ä¼˜åŒ–ã€‚å®ƒä½¿ç”¨ `Selenium` é©±åŠ¨åœ¨åå°è¿è¡Œçš„ `Chrome` æµè§ˆå™¨è·å–é¡µé¢æºç ï¼Œå†ç”± `BeautifulSoup` å’Œ `json` è§£ææ•°æ®ã€‚")
