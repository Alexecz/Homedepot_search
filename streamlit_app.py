import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import json

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (ä»…ä½¿ç”¨ Requests) ---
def scrape_homedepot_with_requests(query, max_pages=1):
    """
    å°è¯•ä»…ä½¿ç”¨ requests å’Œ BeautifulSoup æ¥æŠ“å– Home Depotã€‚
    è¿™ä¸ªæ–¹æ³•æ—¨åœ¨éªŒè¯ç½‘ç«™æ˜¯å¦ä¼šå‘éæµè§ˆå™¨ç¯å¢ƒè¿”å›æ•°æ®ã€‚

    Args:
        query (str): æœç´¢å…³é”®è¯ã€‚
        max_pages (int): è¦æŠ“å–çš„æœ€å¤§é¡µæ•°ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰å•†å“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    all_results = []
    
    # ä½¿ç”¨ä¼šè¯æ¥ä¿æŒ cookies
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    session.headers.update(headers)

    for page_num in range(1, max_pages + 1):
        # æ„é€ å¸¦æœ‰é¡µç çš„æœç´¢URL
        search_url = f"https://www.homedepot.com/s/{query.replace(' ', '%20')}?page={page_num}"
        
        try:
            st.write("---")
            st.write(f"**æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...**")
            st.write(f"  > è¯·æ±‚URL: {search_url}")
            
            response = session.get(search_url, timeout=20)
            response.raise_for_status()
            st.success(f"  > è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")

            soup = BeautifulSoup(response.text, 'html.parser')

            # --- å…³é”®éªŒè¯æ­¥éª¤ ---
            # 1. å°è¯•å¯»æ‰¾æˆ‘ä»¬ä¹‹å‰åœ¨Seleniumä¸­æ‰¾åˆ°çš„ld+jsonæ•°æ®å—
            st.write("  > **ç­–ç•¥1: å¯»æ‰¾ `ld+json` æ•°æ®è„šæœ¬...**")
            script_tag = soup.find('script', {'id': 'thd-helmet__script--browseSearchStructuredData', 'type': 'application/ld+json'})
            
            if script_tag:
                st.success("   > æ‰¾åˆ°äº† `ld+json` æ•°æ®è„šæœ¬ï¼æ­£åœ¨è§£æ...")
                json_data = json.loads(script_tag.string)
                products_list = json_data[0].get('mainEntity', {}).get('offers', {}).get('itemOffered', [])
                
                if products_list:
                    st.write(f"  > **ä» `ld+json` æˆåŠŸè§£æåˆ° {len(products_list)} ä¸ªäº§å“!**")
                    for product in products_list:
                        name = product.get('name', 'N/A')
                        offers = product.get('offers', {})
                        price = offers.get('price', 'N/A') if isinstance(offers, dict) else 'N/A'
                        link = offers.get('url', '#') if isinstance(offers, dict) else '#'
                        image_url = product.get('image')

                        if name != 'N/A':
                            all_results.append({'name': name, 'price': price, 'link': link, 'image_url': image_url})
                else:
                    st.warning("  > `ld+json` è„šæœ¬ä¸­æ²¡æœ‰å•†å“æ•°æ®ã€‚")
            else:
                st.error("  > **æœªæ‰¾åˆ° `ld+json` æ•°æ®è„šæœ¬ã€‚** è¿™æ„å‘³ç€æœåŠ¡å™¨æ²¡æœ‰å‘æˆ‘ä»¬å‘é€åŒ…å«æ•°æ®çš„HTMLã€‚")
                st.info("è¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºï¼Œè¯å®äº†ç½‘ç«™çš„åçˆ¬è™«æœºåˆ¶ã€‚ä¸‹é¢æ˜¯è·å–åˆ°çš„éƒ¨åˆ†HTMLæºç ï¼š")
                st.code(soup.prettify()[:2000], language='html')
                # æ—¢ç„¶æœ€å¯é çš„æ•°æ®æºæ²¡æœ‰ï¼Œå°±æ²¡æœ‰å¿…è¦ç»§ç»­å°è¯•è§£æå…¶ä»–æ ‡ç­¾äº†
                break
            
            # å¦‚æœæ˜¯å¤šé¡µæŠ“å–ï¼Œåˆ™éœ€è¦æ‰¾åˆ°ä¸‹ä¸€é¡µçš„é“¾æ¥
            if page_num < max_pages:
                st.write("  > æ­£åœ¨å¯»æ‰¾ä¸‹ä¸€é¡µçš„é“¾æ¥...")
                # ä»åˆ†é¡µå™¨ä¸­å¯»æ‰¾ä¸‹ä¸€é¡µçš„é“¾æ¥
                pagination = soup.find('nav', {'aria-label': 'Pagination Navigation'})
                if pagination:
                    next_page_element = pagination.find('a', {'aria-label': 'Skip to Next Page'})
                    if next_page_element and next_page_element.has_attr('href'):
                        next_page_url = "https://www.homedepot.com" + next_page_element['href']
                        st.info(f"  > æ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥: {next_page_url}")
                        # åœ¨å¾ªç¯çš„ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªæ–°URL
                        search_url = next_page_url 
                        time.sleep(1) # ç¤¼è²Œæ€§å»¶è¿Ÿ
                    else:
                        st.success("âœ… æœªæ‰¾åˆ° 'Next' æŒ‰é’®ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µï¼ŒæŠ“å–ç»“æŸã€‚")
                        break
                else:
                    st.warning("  > æœªæ‰¾åˆ°åˆ†é¡µå™¨ï¼Œæ— æ³•è¿›è¡Œç¿»é¡µã€‚")
                    break
        
        except requests.exceptions.RequestException as e:
            st.error(f"è¯·æ±‚æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            break
        except Exception as e:
            st.error(f"å¤„ç†ç¬¬ {page_num} é¡µæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            break
            
    return all_results

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="Home Depot Requests çˆ¬è™«", layout="wide")
st.title("ğŸ›’ Home Depot å•†å“æŠ“å–å·¥å…· (çº¯Requestsç‰ˆ)")

search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", "milwaukee")
num_pages = st.number_input("æœ€å¤šæŠ“å–çš„é¡µæ•°:", min_value=1, max_value=5, value=1)

if st.button("ğŸš€ å¼€å§‹æœç´¢"):
    if not search_query:
        st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
    else:
        with st.spinner(f"æ­£åœ¨ä½¿ç”¨ Requests ç›´æ¥è¯·æ±‚é¡µé¢..."):
            scraped_data = scrape_homedepot_with_requests(search_query, num_pages)

        if scraped_data:
            st.success(f"ğŸ‰ **æŠ“å–å®Œæˆï¼å…±è·å¾— {len(scraped_data)} æ¡å•†å“ä¿¡æ¯ï¼**")
            df = pd.DataFrame(scraped_data).drop_duplicates(subset=['name'])
            display_df_data = [{
                "å›¾ç‰‡": row['image_url'] or 'https://placehold.co/100x100/e2e8f0/333333?text=No+Image',
                "å•†å“åç§°": row['name'],
                "ä»·æ ¼": f"${row['price']}" if row.get('price') else 'N/A',
                "é“¾æ¥": row['link']
            } for _, row in df.iterrows()]
            
            st.dataframe(
                display_df_data,
                column_config={
                    "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                    "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                    "ä»·æ ¼": st.column_config.TextColumn("ä»·æ ¼", width="small"),
                    "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                }, hide_index=True, use_container_width=True)
        else:
            st.error("æœªèƒ½é€šè¿‡ç›´æ¥è¯·æ±‚æŠ“å–åˆ°ä»»ä½•å•†å“ä¿¡æ¯ã€‚")
            st.warning("è¿™å¼ºçƒˆè¡¨æ˜ç½‘ç«™å†…å®¹æ˜¯åŠ¨æ€åŠ è½½çš„ï¼Œå¿…é¡»ä½¿ç”¨ **Selenium** æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨æ‰èƒ½æˆåŠŸã€‚")

st.markdown("---")
st.markdown("æŠ€æœ¯è¯´æ˜ï¼šæ­¤åº”ç”¨ä½¿ç”¨ `requests` + `BeautifulSoup` å°è¯•ç›´æ¥è§£æHTMLã€‚")
