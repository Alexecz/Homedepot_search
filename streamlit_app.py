import streamlit as st
import pandas as pd
import time
import json
from bs4 import BeautifulSoup

# --- Selenium ç›¸å…³å¯¼å…¥ --- 
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException

# --- æ ¸å¿ƒæŠ“å–é€»è¾‘ (å…¨è‡ªåŠ¨æ™ºèƒ½ç¿»é¡µ + è¯¦ç»†ä»·æ ¼è§£æ) ---
def scrape_homedepot_with_selenium(query):
    """
    ä½¿ç”¨Seleniumé©±åŠ¨æµè§ˆå™¨ï¼Œé€šè¿‡æ™ºèƒ½ç¿»é¡µæŠ“å–æ‰€æœ‰é¡µé¢ï¼Œå¹¶ä»__NEXT_DATA__ä¸­è§£æè¯¦ç»†ä»·æ ¼ä¿¡æ¯ã€‚

    Args:
        query (str): æœç´¢å…³é”®è¯ã€‚

    Returns:
        list: åŒ…å«æ‰€æœ‰é¡µé¢å•†å“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ã€‚
    """
    search_url = f"https://www.homedepot.com/s/{query.replace(' ', '%20')}"
    all_results = []
    
    status_placeholder = st.empty() # åˆ›å»ºä¸€ä¸ªç”¨äºåŠ¨æ€æ›´æ–°çš„å ä½ç¬¦
    
    driver = None
    try:
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument(f"--user-data-dir=/tmp/selenium_user_data_{int(time.time())}")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        service = Service()
        driver = webdriver.Chrome(service=service, options=options)
        
        status_placeholder.info(f"ğŸš€ æµè§ˆå™¨å·²å¯åŠ¨ï¼Œæ­£åœ¨è®¿é—®åˆå§‹é¡µé¢...")
        driver.get(search_url)

        current_page = 1
        total_pages_str = "?" # é»˜è®¤ä¸ºæœªçŸ¥
        
        while True:
            # æ›´æ–°çŠ¶æ€
            status_text = f"â³ æ­£åœ¨å¤„ç†ç¬¬ {current_page} / {total_pages_str} é¡µ | å·²æŠ“å– {len(all_results)} ä¸ªå•†å“..."
            status_placeholder.info(status_text)
            
            wait = WebDriverWait(driver, 30)
            
            try:
                # 1. ç­‰å¾…å½“å‰é¡µæ•°æ®åŠ è½½
                wait.until(EC.presence_of_element_located((By.ID, "__NEXT_DATA__")))

                # 2. è§£ææ•°æ®
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå°è¯•è·å–æ€»é¡µæ•°
                if current_page == 1:
                    try:
                        page_buttons = soup.select('nav[aria-label="Pagination Navigation"] a[aria-label*="Go to Page"]')
                        if page_buttons:
                            last_page_num = page_buttons[-1].text.strip()
                            if last_page_num.isdigit():
                                total_pages_str = last_page_num
                    except Exception:
                        total_pages_str = "?" 

                script_tag = soup.find('script', {'id': '__NEXT_DATA__', 'type': 'application/json'})
                
                if not script_tag:
                    break

                json_data = json.loads(script_tag.string)
                products_list = []
                page_props = json_data.get('props', {}).get('pageProps', {})
                if page_props.get('search', {}).get('contentLayouts'):
                    content_layouts = page_props['search']['contentLayouts']
                    for layout in content_layouts:
                        if isinstance(layout, dict) and layout.get('type') == 'PRODUCT_POD' and layout.get('products'):
                            products_list.extend(layout.get('products', []))
                
                if not products_list:
                    break
                
                for product in products_list:
                    item_data = product.get('item', {})
                    name = item_data.get('productLabel', 'N/A')
                    pricing_info = item_data.get('pricing', {})
                    
                    # æå–åŸä»·å’Œç°å”®ä»·
                    original_price = pricing_info.get('originalPrice', {}).get('price')
                    special_price = pricing_info.get('specialPrice', {}).get('price')

                    current_price = special_price if special_price else original_price
                    was_price = original_price if special_price and original_price != special_price else None

                    link = 'https://www.homedepot.com' + item_data.get('url', '#')
                    image_url = item_data.get('media', {}).get('images', [{}])[0].get('url')

                    if name != 'N/A':
                        all_results.append({
                            'name': name, 
                            'current_price': current_price,
                            'original_price': was_price,
                            'link': link, 
                            'image_url': image_url or 'https://placehold.co/100x100/e2e8f0/333333?text=No+Image'
                        })

            except TimeoutException:
                status_placeholder.error(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆç¬¬ {current_page} é¡µï¼‰ã€‚")
                break 

            # 3. å¯»æ‰¾ä¸‹ä¸€é¡µçš„URLå¹¶å¯¼èˆª
            try:
                next_page_element = driver.find_element(By.CSS_SELECTOR, 'a[aria-label="Skip to Next Page"]')
                next_page_url = next_page_element.get_attribute('href')
                
                driver.get(next_page_url)
                current_page += 1
                time.sleep(1) # ç¤¼è²Œæ€§å»¶è¿Ÿ

            except NoSuchElementException:
                status_placeholder.success(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ŒæŠ“å–å®Œæˆï¼å…±å¤„ç† {current_page} é¡µã€‚")
                break 
    
    except Exception as e:
        status_placeholder.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        if driver:
            driver.quit()
            
    return all_results

# --- Streamlit åº”ç”¨ç•Œé¢ ---
st.set_page_config(page_title="åœ¨çº¿å•†å“ä¿¡æ¯å·¥å…·", layout="wide")
st.title("ğŸ›’ åœ¨çº¿å•†å“ä¿¡æ¯å·¥å…·")

search_query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯:", "milwaukee")

if st.button("ğŸš€ å¼€å§‹æœç´¢ (æŠ“å–å…¨éƒ¨åˆ†é¡µ)"):
    if not search_query:
        st.warning("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼")
    else:
        all_scraped_data = scrape_homedepot_with_selenium(search_query)

        if all_scraped_data:
            st.success(f"ğŸ‰ **ä»»åŠ¡ç»“æŸï¼å…±è·å¾— {len(all_scraped_data)} æ¡å•†å“ä¿¡æ¯ï¼**")
            
            # åˆ†ç¦»å”¯ä¸€é¡¹å’Œé‡å¤é¡¹
            full_df = pd.DataFrame(all_scraped_data)
            duplicates_mask = full_df.duplicated(subset=['name'], keep='first')
            unique_df = full_df[~duplicates_mask]
            duplicate_df = full_df[duplicates_mask]
            
            st.info(f"å»é‡åå‰©ä½™ {len(unique_df)} æ¡ç‹¬ç«‹å•†å“ä¿¡æ¯ã€‚")
            
            # æ˜¾ç¤ºç‹¬ç«‹å•†å“ä¿¡æ¯
            st.subheader("ç‹¬ç«‹å•†å“ä¿¡æ¯")
            display_unique_data = [{
                "å›¾ç‰‡": row['image_url'],
                "å•†å“åç§°": row['name'],
                "åŸä»·": f"${row['original_price']}" if pd.notna(row['original_price']) else " ",
                "ç°å”®ä»·": f"${row['current_price']}" if pd.notna(row['current_price']) else 'N/A',
                "é“¾æ¥": row['link']
            } for _, row in unique_df.iterrows()]
            
            st.dataframe(
                display_unique_data,
                column_config={
                    "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                    "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                    "åŸä»·": st.column_config.TextColumn("åŸä»·", width="small"),
                    "ç°å”®ä»·": st.column_config.TextColumn("ç°å”®ä»·", width="small"),
                    "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                }, hide_index=True, use_container_width=True)

            # å¦‚æœæœ‰é‡å¤é¡¹ï¼Œåˆ™åœ¨å±•å¼€å™¨ä¸­æ˜¾ç¤º 
            if not duplicate_df.empty:
                with st.expander(f"æŸ¥çœ‹ {len(duplicate_df)} æ¡é‡å¤çš„å•†å“ä¿¡æ¯"):
                    st.subheader("é‡å¤æŠ“å–çš„å•†å“ä¿¡æ¯")
                    display_duplicate_data = [{
                        "å›¾ç‰‡": row['image_url'],
                        "å•†å“åç§°": row['name'],
                        "åŸä»·": f"${row['original_price']}" if pd.notna(row['original_price']) else " ",
                        "ç°å”®ä»·": f"${row['current_price']}" if pd.notna(row['current_price']) else 'N/A',
                        "é“¾æ¥": row['link']
                    } for _, row in duplicate_df.iterrows()]
                    
                    st.dataframe(
                        display_duplicate_data,
                        column_config={
                            "å›¾ç‰‡": st.column_config.ImageColumn("å›¾ç‰‡é¢„è§ˆ", width="small"),
                            "å•†å“åç§°": st.column_config.TextColumn("å•†å“åç§°", width="large"),
                            "åŸä»·": st.column_config.TextColumn("åŸä»·", width="small"),
                            "ç°å”®ä»·": st.column_config.TextColumn("ç°å”®ä»·", width="small"),
                            "é“¾æ¥": st.column_config.LinkColumn("è¯¦æƒ…é“¾æ¥", display_text="ğŸ”— æŸ¥çœ‹å•†å“", width="small")
                        }, hide_index=True, use_container_width=True)
            
        else:
            st.error("æœªèƒ½æŠ“å–åˆ°ä»»ä½•å•†å“ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹çš„æ—¥å¿—åˆ†æåŸå› ã€‚")
